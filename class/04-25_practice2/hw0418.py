# -*- coding: utf-8 -*-
"""hw0418.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1COOMDCyJJmK1fsUgJsUEx5t3_emC5vNb

**ä¸€ã€è¼‰å…¥å¥—ä»¶**
"""

import pandas as pd
import numpy as np
from sklearn.tree import plot_tree
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, roc_curve, auc
import matplotlib.pyplot as plt
from matplotlib import font_manager as fm

"""ä¸­æ–‡å­—é«”"""

# @title
# è£œcolabä¸­æ–‡å­—é«”
!apt-get update -y
!apt-get install -y fonts-noto-cjk

# @title
# æŒ‡å®šå­—é«”æª”è·¯å¾‘
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
font_path = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'
font_prop = fm.FontProperties(fname=font_path, size=14)

"""**äºŒã€é€£æ¥é›²ç«¯**"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

file_path = '/content/drive/MyDrive/ç›£ç£å¼kidneyDisease/kidney_disease.csv'
df = pd.read_csv(file_path)

"""**ä¸‰ã€è³‡æ–™è™•ç†**"""

# å»æ‰æ¬„ä½åç¨±çš„å‰å¾Œç©ºç™½
df.columns = df.columns.str.strip()

# çœ‹æ¯æ¬„ç¼ºå¤±å€¼æ•¸é‡
print(df.isnull().sum())

# è™•ç† target æ¬„
target_col = 'Chronic Kidney Disease: yes'
df[target_col] = pd.to_numeric(df[target_col], errors='coerce')

# æ•¸å€¼åŒ– + è£œå€¼
df = df.apply(lambda col: pd.to_numeric(col, errors='coerce') if col.dtype == 'object' else col) #æ¬„ä½->æ•¸å­—
df.fillna(df.mean(numeric_only=True), inplace=True)

# ç§»é™¤å¤ªå¼·çš„ç‰¹å¾µï¼ˆæ´©æ¼é¢¨éšªï¼‰
drop_cols = ['Hemoglobin (gms)', 'Specific Gravity', 'Packed Cell Volume','Serum Creatinine (mgs/dL)','Red Blood Cells (millions/cmm)'] #ç§»é™¤æ¬Šç¨®éåŠçš„
df = df.drop(columns=drop_cols)

"""**å››ã€æ¨¡å‹è³‡æ–™è¼¸å…¥**"""

# ç‰¹å¾µ/æ¨™ç±¤
X = df.drop(columns=[target_col])
y = df[target_col]

# è³‡æ–™åˆ‡åˆ†ï¼ˆåŠ å…¥ stratifyï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**æ±ºç­–æ¨¹å±¤æ•¸åˆ¤æ–·**"""

# æ¸¬è©¦çš„æ¨¹æ·±åº¦ç¯„åœ
depth_range = range(1, 11)
cv_scores = []

# æ¸¬è©¦ä¸åŒæ·±åº¦çš„æ¨¡å‹è¡¨ç¾ (ä½¿ç”¨è™•ç†å¾Œçš„å¯¦éš›è³‡æ–™ X å’Œ y)
for depth in depth_range:
    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)
    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# ç¹ªè£½ä¸åŒæ·±åº¦çš„äº¤å‰é©—è­‰åˆ†æ•¸
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(depth_range, cv_scores, marker='o')
plt.title("ä¸åŒ max_depth çš„äº¤å‰é©—è­‰æº–ç¢ºç‡", fontproperties=font_prop)
plt.xlabel("max_depth")
plt.ylabel("å¹³å‡æº–ç¢ºç‡", fontproperties=font_prop)
plt.grid(True)

# ç¹ªè£½ä¸åŒ max_depth çš„å­¸ç¿’æ›²ç·š
plt.subplot(1, 2, 2)
for depth in [3, 4, 5]:  # é¸æ“‡å¹¾å€‹ä½ æ„Ÿèˆˆè¶£çš„æ·±åº¦é€²è¡Œæ¯”è¼ƒ
    train_sizes, train_scores, test_scores = learning_curve(
        DecisionTreeClassifier(max_depth=depth, random_state=42),
        X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 5), scoring='accuracy'
    )
    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    plt.plot(train_sizes, train_scores_mean, 'o-', label=f"è¨“ç·´ (depth={depth})")
    plt.plot(train_sizes, test_scores_mean, 'o-', label=f"é©—è­‰ (depth={depth})")

plt.title("ä¸åŒ max_depth çš„å­¸ç¿’æ›²ç·š", fontproperties=font_prop)
plt.xlabel("è¨“ç·´æ¨£æœ¬æ•¸", fontproperties=font_prop)
plt.ylabel("æº–ç¢ºç‡", fontproperties=font_prop)
plt.legend(prop=font_prop)
plt.grid(True)

plt.tight_layout()
plt.show()

"""**äº”ã€å»ºç«‹æ¨¡å‹**"""

# æ¨¡å‹å»ºæ§‹
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42)
}

roc_data = {}
accuracies = {}
# è¨“ç·´ä¸€æ£µæ±ºç­–æ¨¹
tree = DecisionTreeClassifier(max_depth=5, random_state=42)
tree.fit(X_train, y_train)
# æ¨¡å‹è¨“ç·´ + è©•ä¼°
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    auc_score = auc(fpr, tpr)

    accuracies[name] = acc
    roc_data[name] = {'fpr': fpr, 'tpr': tpr, 'auc': auc_score}

#å»ºç«‹èˆ‡è¨“ç·´æ¨¡å‹
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

"""**å…­ã€å–å¾—è³‡æ–™ç‰¹å¾µé‡è¦æ€§ï¼ˆç™¼ç¾æ¬Šé‡éåŠæ¬„ä½å‰‡å›åˆ°ä¸‰ã€åˆªé™¤è©²æ¬„ä½ï¼‰ **"""

# å–å¾—ç‰¹å¾µé‡è¦æ€§
feature_importances = model.feature_importances_

# æ•´ç†æˆ DataFrame, ä½¿ç”¨ X_dropped.columns ä¾†ç¢ºä¿é•·åº¦ä¸€è‡´
importance_df = pd.DataFrame({
    'ç‰¹å¾µ':  X.columns,  # æ”¹ç”¨ X_dropped.columnsï¼Œèˆ‡ feature_importances é•·åº¦ä¸€è‡´
    'é‡è¦æ€§': feature_importances
})

# æ’åº
importance_df = importance_df.sort_values(by='é‡è¦æ€§', ascending=False).reset_index(drop=True)

# è¨ˆç®—ç¸½é‡è¦æ€§ä½”æ¯”
total_importance = importance_df['é‡è¦æ€§'].sum()
importance_df['ä½”æ¯” (%)'] = (importance_df['é‡è¦æ€§'] / total_importance) * 100

# é¡¯ç¤ºçµæœ
print("\nğŸ“Š ç‰¹å¾µé‡è¦æ€§èˆ‡ä½”æ¯”ï¼š")
print(importance_df)

# å»é™¤å‰å…©å (å¦‚æœéœ€è¦)
# importance_df = importance_df.iloc[2:].reset_index(drop=True)

# ç•«é•·æ¢åœ–
plt.figure(figsize=(12, 6))
bars = plt.barh(importance_df['ç‰¹å¾µ'], importance_df['é‡è¦æ€§'], color='teal')

# æ¨™ä¸Šæ¨™é¡Œè·ŸXè»¸æ–‡å­—
plt.xlabel('é‡è¦æ€§',fontproperties=font_prop)
plt.title('ç‰¹å¾µé‡è¦æ€§é•·æ¢åœ– ', fontproperties=font_prop)

# åéä¾†é¡¯ç¤ºï¼Œæœ€å¤§åœ¨ä¸Šé¢
plt.gca().invert_yaxis()

# åŠ ä¸Šæ©«ç·š
plt.grid(axis='x', linestyle='--', alpha=0.6)

# æ¯å€‹é•·æ¢æ—é‚ŠåŠ ä¸Šæ•¸å€¼
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2,
             f'{bar.get_width():.4f}',  # å°æ•¸é»å¾Œå››ä½
             va='center', ha='left', fontproperties=font_prop, fontsize=12)

# è¨­å®šä¸­æ–‡å­—å‹
plt.xticks(fontproperties=font_prop)
plt.yticks(fontproperties=font_prop)

# è®“åœ–ä¸æœƒè·‘æ‰
plt.tight_layout()

# é¡¯ç¤ºåœ–
plt.show()

"""**ä¸ƒã€ç•«åœ–**"""

# === ROC æ›²ç·šåœ– ===
plt.figure(figsize=(8, 6))
for name, data in roc_data.items():
    plt.plot(data['fpr'], data['tpr'], label=f"{name} (AUC = {data['auc']:.2f})")

plt.plot([0, 1], [0, 1], 'k--', label='éš¨æ©ŸçŒœæ¸¬')
plt.xlabel('å‡é™½æ€§ç‡ (FPR)',fontproperties=font_prop)
plt.ylabel('çœŸé™½æ€§ç‡ (TPR)',fontproperties=font_prop)
plt.title('ROC æ›²ç·šæ¯”è¼ƒ',fontproperties=font_prop)
plt.legend(loc='lower right',prop=font_prop)
plt.grid(True)
plt.show()
# === æº–ç¢ºç‡ + AUC é•·æ¢åœ– ===
labels = list(models.keys())
acc_vals = [accuracies[name] for name in labels]
auc_vals = [roc_data[name]['auc'] for name in labels]

x = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots(figsize=(8, 5))
rects1 = ax.bar(x - width/2, acc_vals, width, label='Accuracy', color='skyblue')
rects2 = ax.bar(x + width/2, auc_vals, width, label='AUC', color='lightgreen')

ax.set_ylabel('åˆ†æ•¸',fontproperties=font_prop)
ax.set_title('æ¨¡å‹æº–ç¢ºç‡èˆ‡ AUC æ¯”è¼ƒ',fontproperties=font_prop)
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.set_ylim(0, 1.05)
ax.legend()
ax.grid(axis='y', linestyle='--', alpha=0.6)

# åŠ ä¸Šæ•¸å€¼æ¨™ç±¤
for rect in rects1 + rects2:
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 0.02,
            f'{height:.2f}', ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

#ç•«æ±ºç­–æ¨¹
plt.figure(figsize=(20, 10))
plot_tree(
    tree,
    feature_names=X.columns,
    class_names=['No CKD', 'CKD'],
    filled=True,
    rounded=True,
    fontsize=12
)
plt.title("Decision Tree å¯è¦–åŒ–",fontproperties=font_prop)
plt.show()



#é æ¸¬èˆ‡æ··æ·†çŸ©é™£
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

y_pred = model.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.grid(False)
plt.show()

"""===================æ¸¬è©¦å¾…æ•´ç†===========================

"""